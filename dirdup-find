#!/usr/bin/env python3

import os
import sys
import socket
import hashlib
import re
import time
import gzip

from os.path import getsize, join
from functools import partial

import argparse

l = 0

def processpairs(pairs):
	sortedpairs = sorted(pairs, key=lambda x: x[1], reverse=True);

	d={};
	prev=();
	for e in sortedpairs:
		md5=e[1]
		if md5 != prev:
			d[md5] = [];
			d[md5].append(e[0]);
		if md5 == prev:
			d[md5].append(e[0]);
		prev=md5

	d2={}
	for i in d:
		if(len(d[i]) > 1):
			d2[i] = d[i]

	return d2


def identicalfiles(data):

	pairs=[]
	for line in data:
		pairs.append(((line[5], line[6], line[3]), (int(line[0]), int(line[1]), line[2], line[4])));

	return processpairs(pairs)

def tprint(msg):
	return
	global l
	print("=== PROFILING === %s: %f" % (msg, time.time() - l))
	l = time.time()

def dupdirs_wholetrees(data):

	pairs=[]

	dirs={}
	for line in data:
		input_size, input_time, input_hash, input_path, input_file, input_index=line[0:6]
		dir_components=input_path.split('/')
		path=""
		for c in dir_components:
			if c != "." and path != "":
				path=path + '/' + c
			else:
				path=c

			index=(line[5], line[6], path)

			if (index not in dirs):
				dirs[index]=[];

			dirs[index].append((input_size, input_time, input_hash, input_file))

	for d in dirs:
		pairs.append((d, tuple(dirs[d])));
			
	processedpairs = processpairs(pairs)

	#Clean away cases of directories that only have one subdirectory and
	#thus they appear to be duplicates.
	keys_to_pop=[]
	for p in processedpairs:
		what_to_remove=[]
		for d in processedpairs[p]:
			for e in processedpairs[p]:
				#print("Testing :" + d + ": against :" + e + ":")
				if d[0] == e[0] and d[1] == e[1] and d[2] in e[2] and d[2] != e[2]:
					#print("True!")
					what_to_remove.append(e)
		for w in what_to_remove:
			if w in processedpairs[p]:
				#print("Removing " + w)
				processedpairs[p].remove(w)
				if len(processedpairs[p]) == 1:
					#print("Only one left")
					keys_to_pop.append(p)

	for k in keys_to_pop:
		#print("Deleting!")
		#print(processedpairs[k])
		processedpairs.pop(k)

	#Clean out subdirectories of duplicate parent directories.
#	keys_to_pop=[]
#	for x in processedpairs:
#		for y in processedpairs:
#			if set(x).issubset(set(y)) and set(x) != set(y) and x != y:
#				keys_to_pop.append(x)
#	tprint("general deduplication")

#	for k in keys_to_pop:
#		if k in processedpairs:
#			processedpairs.pop(k)
#	tprint("general deduplication 2")

	for k in processedpairs:
		processedpairs[k] = [processedpairs[k], (sum([x[0] for x in k]),len(k))]


	return processedpairs


def gethostname():
	hostname=socket.gethostname()

	if "DIRDUP_HOSTNAME" in os.environ:
		hostname=os.environ["DIRDUP_HOSTNAME"]

	return hostname



def getworkdir():
	workdir="./"

	if "DIRDUP_WORKDIR" in os.environ:
		workdir=os.environ["DIRDUP_WORKDIR"]

	return workdir


# s must be in the form device:storage_name
def getlocationforstorage(s):
	device, storage_name = s.split(':')[0:2]

	return open('accesses/' + device + '/' + device + '/' + storage_name, 'r').readlines()[0].rstrip('\n').lstrip("file:")


# s must be in the form device:storage_name
def getindexlocationforstorage(s):
	device, storage_name = s.split(':')[0:2]

	return 'indexes/' + device + '/' + storage_name


def getallstorages():
	allstorages=[]
	for root, dirs, files in os.walk('accesses'):
		devices=dirs[:]
		for d in dirs:
			dirs.pop(0)
		for d in devices:
			for rs, ds, fs in os.walk('accesses/' + d + '/' + d):
				allstorages.extend([d + ':' + x for x in fs])

	return allstorages

def getstoragesforhost(d):
	storages=[]
	for rs, ds, fs in os.walk('accesses/' + d + '/' + d):
		storages.extend([d + ':' + x for x in fs])

	return storages

def main(argv):

	parser = argparse.ArgumentParser(
	   description='Find duplicate elements in the dirdup indexes.'
	)
	parser.add_argument('--type', choices=['f','d','ff','dd'], default='f',
	   help='finds files, directories, dupfiles, or dupdirectories (default: files)'
	)
	parser.add_argument('storages', default=['@ALL'], nargs='*',
	   help='what storages to process (or @ALL or @LOCAL)'
	)

	args = parser.parse_args()

	hostname=gethostname()

	storages=set()
	if '@ALL' in args.storages:
		storages |= set(getallstorages())

	if '@LOCAL' in args.storages:
		storages |= set(getstoragesforhost(hostname))

	storages |= set(set(args.storages) - {'@ALL', '@LOCAL'})


	indexes={}
	for s in storages:
		indexlocation=getindexlocationforstorage(s)
		if indexlocation not in indexes:
			indexes[indexlocation]=tuple(s.split(':'))


	workdir=getworkdir()

	readdata=[]
	for f in indexes:
		for l in gzip.open(f + '.gz', 'r').readlines():
			line=l.decode("utf-8")
			if (line == '\n'):
				continue
			#print(line)
			fields=re.split(' +', line.rstrip('\n').lstrip(' '), 3)
			file_sections = fields[3].rpartition('/')
			readdata.append((int(fields[0]), int(fields[1]), fields[2], file_sections[0], file_sections[2], indexes[f][0], indexes[f][1]))


	if args.type == 'ff':
		duplicatefiles=identicalfiles(readdata)
		for key in sorted(duplicatefiles, reverse=True):
			print("%s (size=%d, mtime=%d) found in:" % (key[3], key[0], key[1]))
			for s in sorted(duplicatefiles[key]):
				print(":", s)

	elif args.type == 'dd':
		duplicatedirs=dupdirs_wholetrees(readdata)
		sorteddupdirs=sorted(duplicatedirs, reverse=True, key=lambda x: duplicatedirs[x][1][0]*duplicatedirs[x][1][1])
		for key in sorteddupdirs:
			print("Directory with duplicate content (size=%d in %d files) found:" % (duplicatedirs[key][1][0], duplicatedirs[key][1][1]))
			for s in sorted(duplicatedirs[key][0]):
				print(":", s)


if __name__ == '__main__':
	main(sys.argv)
